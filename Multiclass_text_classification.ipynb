{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heesukjang/W266_NLP_With_DeepLearning/blob/main/Multiclass_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-44Qz4-Xd1o"
      },
      "source": [
        "# Assignment 3: Fine tuning a multiclass classification BERT model\n",
        "\n",
        "**Description:** This assignment covers fine-tuning of a multiclass classification. You will compare two different types of solutions using BERT-based models. You should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "* Working with BERT\n",
        "* Using multiple models to focus on different sub-tasks\n",
        "* Different metrics to measure the effectiveness of your model\n",
        "\n",
        "\n",
        "\n",
        "The assignment notebook closely follows the lesson notebooks. We will use the 20 newsgroups dataset and will leverage some of the models, or part of the code, for our current investigation. \n",
        "\n",
        "**You are strongly encouraged to read through the entire notebook before answering any questions or writing any code.**\n",
        "\n",
        "The initial part of the notebook is purely setup. We will then generate our BERT model and see if and how we can improve it. \n",
        "\n",
        "Do not try to run this entire notebook on your GCP instance as the training of models requires a GPU to work in a timely fashion. This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1h.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2023-spring-main/blob/master/assignment/a3/Multiclass_text_classification.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "1. Setup\n",
        "  \n",
        "  1.1 Libraries & Helper Functions\n",
        "\n",
        "  1.2 Data Acquisition\n",
        "\n",
        "  1.3 Training/Test/Validation Sets for BERT-based models\n",
        "\n",
        "2. Classification with a fine tuned BERT model\n",
        "\n",
        "  2.1 Create the specified BERT model\n",
        "  \n",
        "  2.2 Fine tune the BERT model as directed\n",
        "\n",
        "  2.3 Examine the predictions with various metrics\n",
        "\n",
        "3. Classification using two stages\n",
        "\n",
        "  3.1 Relabel the data to group the often confused classes\n",
        "\n",
        "  3.2 Train the first stage model on the relabeled data\n",
        "\n",
        "  3.3 Separate the data for just the confused classes\n",
        "\n",
        "  3.4. Train the second stage model on the two classes\n",
        "\n",
        "  3.5. Combine and evaluate the predictions from the two stages\n",
        "\n",
        "4. Look at examples of misclassifications, see what might have changed\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**: \n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1 and a2.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* If you want to, you can run all of the cells in section 1 in bulk. This is setup work and no questions are in there. At the end of section 1 we will state all of the relevant variables that were defined and created in section 1.\n",
        "\n",
        "* **IMPORTANT NOTE:** Because the data we're using is downloaded each time we run section 1, a different split of train, validation, and test records is created.  This means that the accuracy, precision, recall, and F1 scores will change, although the delta will be small.  Please enter the values from your final run so that the answer values in your answers file correspond to the answer values in the outputs in your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK2xTV-Xisrl"
      },
      "source": [
        "### 1. Setup\n",
        "\n",
        "Lets get all our libraries and download and process our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3gzOXaFdmgO8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULcKJUjHmkZT",
        "outputId": "0264403b-1ef3-4c24-a3a5-fe6c5d180530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aTq7Qjqbmkfv"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-6E-xzDawK0Q"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "99yIOuqFXMwU"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification (Name Entity Tagging) with BERT (QUESTION ???)\n",
        "[The 20 newsgroups text dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
        "\n",
        "This module contains two loaders. The first one, sklearn.datasets.**fetch_20newsgroups**, <u>returns a list of the raw texts</u> that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor."
      ],
      "metadata": {
        "id": "xgjGqU4J0FHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UWk_PZ9hmkk0"
      },
      "outputs": [],
      "source": [
        "def read_20newsgroups(test_size=0.1):\n",
        "  # download & load 20newsgroups dataset from sklearn's repos\n",
        "  dataset = fetch_20newsgroups(subset=\"all\", shuffle=True, remove=(\"headers\", \"footers\", \"quotes\"))\n",
        "  documents = dataset.data\n",
        "  labels = dataset.target\n",
        "  # split into training & testing a return data as well as label names\n",
        "  return train_test_split(documents, labels, test_size=test_size), dataset.target_names\n",
        "  \n",
        "# call the function\n",
        "(train_texts, test_texts, train_labels, test_labels), target_names = read_20newsgroups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ocyl9dnb1OM"
      },
      "source": [
        "Take a look at the records.  We basically have a long string of text and an associated label.  That label is the Usenet group where the posting occured. The records are the raw text.  They vary significantly in size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "O7Akxu2Umkpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe00a706-2024-48b9-f032-a69a5c0a7f5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Heise model 710A pressure meter. This is a precision 4-1/2\\ndigit meter measuring 0 - 15 PSI (absolute) in .001 psi\\nincrements.  Case is in extremely good shape, and can be used\\nas a stand-alone meter or panel mounted. Brass fitting (looks\\nlike standard 3/8\") on back. Operates from 110 VAC.\\n\\nI\\'d like $50 for it, or make an offer. It is a lot more useful to\\na lab than as an ersatz barometer, which is what I\\'ve been using\\nit for.\\n\\n-- \\n--------------------------------------------------------------------\\n       Dave Medin\\t\\t\\tPhone:\\t(205) 730-3169 (w)\\n    SSD--Networking\\t\\t\\t\\t(205) 837-1174 (h)\\n    Intergraph Corp.\\n       M/S GD3004 \\t\\tInternet: dtmedin@catbyte.b30.ingr.com\\n  Huntsville, AL 35894\\t\\tUUCP:  ...uunet!ingr!b30!catbyte!dtmedin\\n\\n   ******* Everywhere You Look (at least around my office) *******',\n",
              " 'Hi. I\\'ve just finished reading S414, and have several questions about\\nthe Brady bills (S414 and HR1025).\\n\\n1. _Are_ these the current versions of the Brady bill?\\n     What is the status of these bills?  I\\'ve heard they\\'re \"in committee\".\\n     How close is that to being made law?\\n\\n2. S414 and HR1025 seem fairly similar.  Are there any important\\n   differences I missed?\\n\\n3. S414 seems to have some serious loopholes:\\n  A. S414 doesn\\'t specify an \"appeals\" process to wrongful denial during\\n     the waiting period, other than a civil lawsuit(?)  (S414 has an appeals\\n     process once the required instant background check system is established,\\n     but not before).\\n  B. the police are explicitly NOT liable for mistakes in denying/approving\\n     using existing records (so who would I sue in \"A\" above to have an\\n     inaccurate record corrected?)\\n  C. S414 includes an exception-to-waiting-period clause for if a person\\n     can convince the local Chief Law-Enforcement Officer (CLEO) of an\\n     immediate threat to his or her life, or life of a household member.\\n     But S414 doesn\\'t say exactly what is considered a \"threat\", nor does\\n     it place a limit on how long the CLEO takes to issue an exception\\n     statement.\\nTrue?  Have I misunderstood?  Any other \\'holes?\\n\\n4. With just S414, what\\'s to stop a person with a \"clean\" record from\\n   buying guns, grinding off the serial numbers, and selling them to crooks?\\n   At minimum, what additional laws are needed to prevent this?\\n\\n   \\'Seems at min. a \"gun counting\" scheme would be needed\\n   (e.g., \"John Doe owns N guns\").  So, if S414 passes, I wouldn\\'t be surprised\\n   to see legislation for stricter, harder-to-forge I.D.\\'s plus national gun\\n   registration, justified by a need to make the Brady bill work.\\n\\nPlease comment.  I\\'m mainly interested in specific problems with the current\\nlegislation--I don\\'t mean to start a general discussion of the merits\\nof any/all waiting-period bills ever proposed.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "train_texts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eKIgSBdcHy9"
      },
      "source": [
        "Notice the \"labels\" are just integers that are an offset into the list of target names. (QUESTION: NOT UNDERSTANDING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KRnu9CSQnMNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ecc2c5-a5a9-4088-d5f0-87454395e13f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# the 1st raw text is labeled as \"rec.autos\" at index 7 \n",
        "# the 2nd raw text is labeled as \"talk.politics.mideast\" at index 17 ??? \n",
        "train_labels[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ksEnmbcWBt"
      },
      "source": [
        "The variable ''target_names'' stores all of the names of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qvNAnCstx-3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b5f774-700c-44c8-da9c-f53f77ec8306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "0 alt.atheism\n",
            "1 comp.graphics\n",
            "2 comp.os.ms-windows.misc\n",
            "3 comp.sys.ibm.pc.hardware\n",
            "4 comp.sys.mac.hardware\n",
            "5 comp.windows.x\n",
            "6 misc.forsale\n",
            "7 rec.autos\n",
            "8 rec.motorcycles\n",
            "9 rec.sport.baseball\n",
            "10 rec.sport.hockey\n",
            "11 sci.crypt\n",
            "12 sci.electronics\n",
            "13 sci.med\n",
            "14 sci.space\n",
            "15 soc.religion.christian\n",
            "16 talk.politics.guns\n",
            "17 talk.politics.mideast\n",
            "18 talk.politics.misc\n",
            "19 talk.religion.misc\n"
          ]
        }
      ],
      "source": [
        "print(target_names)\n",
        "\n",
        "for idx, val in enumerate(target_names[:20]):\n",
        "  print(idx, val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1SmEVYOcwP6"
      },
      "source": [
        "We already have a test set and a train set.  Let's explicitly set aside part of our training set for validation purposes. (split the given training further into training and validation set) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "H1toWirQAAZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd87369-cc33-4086-c3fb-161a3df6a72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_texts) = 16961\n",
            "len(valid_texts) = 961\n",
            "len(valid_labels) = 961\n",
            "len(train_texts) = 16000\n",
            "len(train_texts) = 16000\n"
          ]
        }
      ],
      "source": [
        "print(f'len(train_texts) = {len(train_texts)}')\n",
        "valid_texts = train_texts[16000:]\n",
        "valid_labels = train_labels[16000:]     # set about 5% of the total training set to be validation set\n",
        "train_texts = train_texts[:16000]\n",
        "train_labels = train_labels[:16000]\n",
        "\n",
        "print(f'len(valid_texts) = {len(valid_texts)}\\nlen(valid_labels) = {len(valid_labels)}')\n",
        "print(f'len(train_texts) = {len(train_labels)}\\nlen(train_texts) = {len(train_labels)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBMiGVwHc76-"
      },
      "source": [
        "The validation set will always have 961 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bzuXsJjwAAOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801d2834-331f-4df6-9072-847b81dc5193"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "961"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "len(valid_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBy5EKYwdBcZ"
      },
      "source": [
        "The training set will always have 16000 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MMVgiD8SCHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba92d17-5430-436c-b1a1-5d4ff9b6b52d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "len(train_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGVOwMBKu2OW"
      },
      "source": [
        "**NOTE:** Each time you rerun the data you will draw a *different* set of train and test documents even though the numbers 961 and 16000 will always be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mFISqjoM335H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7b3de9-f9c0-4294-b0ba-50deab5504c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "961 [ 3  1 13 10 10  1  9 15 14  4]\n"
          ]
        }
      ],
      "source": [
        "#get the labels in a needed data format for validation\n",
        "npvalid_labels = np.asarray(valid_labels)   \n",
        "print(len(npvalid_labels), npvalid_labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3T5uytRdK7I"
      },
      "source": [
        "* train_texts - an array of text strings for training\n",
        "* test_texts - an array of text strings for testing \n",
        "* valid texts - an array of text strings for validation\n",
        "* train_labels - an array of integers representing the labels associated with train_texts\n",
        "* test_labels - an array of integers representing the labels associated with test_texts\n",
        "* valid_labels - an array of integers representing the labels associated with valid_texts\n",
        "* target_names - an array of label strings that correspond to the integers in the *_labels arrays\n",
        "\n",
        "### 2. Classification with a fine tuned BERT model\n",
        "\n",
        "Let's pick our BERT model.  We'll start with the base BERT model and we'll use the cased version since our data has capital and lower case letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zjFWaM2ZnMIC"
      },
      "outputs": [],
      "source": [
        "#make it easier to use a variety of BERT subword models\n",
        "model_checkpoint = 'bert-base-cased'   # case sensitive (care about upper and lower case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-7gV_GUdn9Ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c56d87-4ace-4438-807b-f2f5d5973644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "bert_model = TFBertModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJGkIHSHdilH"
      },
      "source": [
        "We're setting our maximum training record length to 200.  BERT models can handle more and after you've completed the assignment you're welcome to try larger and small sized records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "h_59AaVznMCV"
      },
      "outputs": [],
      "source": [
        "max_length = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6d54_bqd58L"
      },
      "source": [
        "Now we'll tokenize our three data slices.  This will take a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bgv0nftinL6z"
      },
      "outputs": [],
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`, \n",
        "# and pad with 0's when less than `max_length` and return a tf Tensor\n",
        "train_encodings = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gNS0Fi0emN8"
      },
      "source": [
        "Notice our input_ids for the first training record and their padding. The train_encodings also includes an array of token_type_ids and an attention_mask array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XFEXxgAmnLve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e742682-4291-4794-8426-a602e7105a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_encodings.input_ids[:1]) =  1 \n",
            "train_encodings.input_ids[:1] tf.Tensor(\n",
            "[[  101  1124  4862  2235  5729  1568  1592  2997  8227   119  1188  1110\n",
            "    170 13218   125   118   122   120   123 16521  8227 10099   121   118\n",
            "   1405 12727  2240   113  7846   114  1107   119  3135  1475 15604  1182\n",
            "   1107 13782  4385   119  9060  1110  1107  4450  1363  3571   117  1105\n",
            "   1169  1129  1215  1112   170  2484   118  2041  8227  1137  5962  5378\n",
            "    119 20982 11732   113  2736  1176  2530   124   120   129   107   114\n",
            "   1113  1171   119  5434  3052  1121  6745 19497  1658   119   146   112\n",
            "    173  1176   109  1851  1111  1122   117  1137  1294  1126  2906   119\n",
            "   1135  1110   170  1974  1167  5616  1106   170  8074  1190  1112  1126\n",
            "  14044 28027  1584  2927 20182   117  1134  1110  1184   146   112  1396\n",
            "   1151  1606  1122  1111   119   118   118   118   118   118   118   118\n",
            "    118   118   118   118   118   118   118   118   118   118   118   118\n",
            "    118   118   118   118   118   118   118   118   118   118   118   118\n",
            "    118   118   118   118   118   118   118   118   118   118   118   118\n",
            "    118   118   118   118   118   118   118   118   118   118   118   118\n",
            "    118   118   118   118   118   118   118   118   118   118   118   118\n",
            "    118   118   118  4111  2508  7126 26385   102]], shape=(1, 200), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print('len(train_encodings.input_ids[:1]) = ',len(train_encodings.input_ids[:1]), '\\ntrain_encodings.input_ids[:1]',train_encodings.input_ids[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23lwfDw2T6gI"
      },
      "source": [
        "### Write a function to **create this multiclass bert model**.\n",
        "\n",
        "Keep in mind the following:\n",
        "* Each record can have one of n labels where **n = the size of target_names**.\n",
        "* We'll still want a **hidden size layer of size 201**\n",
        "* We'll want our hidden layer to make use of the **pooler output** from BERT\n",
        "* We'll also want to use **dropout**\n",
        "* Our **classification layer** will need to be appropriately sized and use the correct non-linearity for a multi-class problem.\n",
        "* Since we have multiple labels we can no longer use binary cross entropy.  Instead we need to change our loss metric to a **categorical cross entropy**.  Which of the two categorical cross entropy metrics will work best here? \n",
        "* Make sure that training affects **all** of the layers in BERT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "RtWMrLi4tIun"
      },
      "outputs": [],
      "source": [
        "# we want to use \"sparse_categorical_crossentropy (integer-based loss function)\" \n",
        "#   compared to just \"categorial_crossentropy (one-hot encoding based loss i.e.)\"\n",
        "def create_bert_multiclass_model(checkpoint = model_checkpoint,\n",
        "                                 num_classes = 20,\n",
        "                                 hidden_size = 201, \n",
        "                                 dropout=0.3,\n",
        "                                 learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint)                                              \n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(200,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    pooler_output = bert_out[1]    # one vector for each \n",
        "    \n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_output)\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, activation='softmax',name='classification_layer')(hidden)    \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),          # A 'sigmoid' activation layer turns the output \"logit\" into a value, 0-1 \n",
        "                                 metrics='accuracy')  \n",
        "    \n",
        "\n",
        "\n",
        "    num_params = model.count_params()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "uR-Id158tIlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859677be-694b-4f19-c346-d12225d249ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "pooler_bert_model = create_bert_multiclass_model(checkpoint=model_checkpoint, num_classes=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "bMxpO16-tIaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbfa840-77f1-4639-eb5f-0c69b9e76812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask_layer (InputLay  [(None, 200)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " input_ids_layer (InputLayer)   [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids_layer (InputLay  [(None, 200)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " tf_bert_model_3 (TFBertModel)  TFBaseModelOutputWi  108310272   ['attention_mask_layer[0][0]',   \n",
            "                                thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
            "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
            "                                n_state=(None, 200,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 201)          154569      ['tf_bert_model_3[0][1]']        \n",
            "                                                                                                  \n",
            " dropout_148 (Dropout)          (None, 201)          0           ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            " classification_layer (Dense)   (None, 20)           4040        ['dropout_148[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,468,881\n",
            "Trainable params: 108,468,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "pooler_bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5chyZZnHdlf9"
      },
      "source": [
        "**QUESTION:** 2.1 How many trainable parameters are in your dense hidden layer?\n",
        "\n",
        "**QUESTION:** 2.2 How many trainable parameters are in your classification layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe3tTWX6FVZL"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(pooler_bert_model, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLLjb3mOtICy"
      },
      "outputs": [],
      "source": [
        "#It takes 10 to 14 minutes to complete an epoch when using a GPU\n",
        "pooler_bert_model_history = pooler_bert_model.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask], \n",
        "                                                  npvalid_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdHlOd4LJUOH"
      },
      "source": [
        "Now we need to run evaluate against our fine-tuned model.  This will give us an overall accuracy based on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPQn2tJPEGdU"
      },
      "outputs": [],
      "source": [
        "#batch 8, ML=201\n",
        "score = pooler_bert_model.evaluate([test_encodings.input_ids, test_encodings.token_type_ids, test_encodings.attention_mask], \n",
        "                                                  test_labels) \n",
        "\n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CnyZsXLfLJt"
      },
      "source": [
        "**QUESTION:** 2.3 What is the Test accuracy score you get from your model? (Just copy and paste the value into the answers sheet and round to five significant digits.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS7Gp_IYEiJ_"
      },
      "outputs": [],
      "source": [
        "#run predict for the first three elements in the test data set\n",
        "predictions = pooler_bert_model.predict([test_encodings.input_ids[:3], test_encodings.token_type_ids[:3], test_encodings.attention_mask[:3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymGM8QnpchnC"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ahMJ-zwEiCs"
      },
      "outputs": [],
      "source": [
        "#run and capture all predictions from our test set using model.predict\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE\n",
        "\n",
        "#now we need to get the highest probability in the distribution for each prediction\n",
        "#and store that in a tf.Tensor\n",
        "predictions_model1 = tf.argmax(predictions_model1, axis=-1)\n",
        "predictions_model1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGI8oA6fMWTI"
      },
      "source": [
        "There are two ways to see what's going on with our classifier.  Overall accuracy is interesting but it can be misleading.  We need to make sure that each of our categories' prediction performance is operating at an equal or higher level than the overall.\n",
        "\n",
        "Here we'll use the classification report from scikit learn.  It expects two inputs as arrays.  One is the ground truth (y_true) and the other is the associated prediction (y_pred).  This is based on gethering all the predictions from our our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPqioIbh2XIE"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, predictions_model1.numpy(), target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddMMQ2vBOJKj"
      },
      "source": [
        "**QUESTION:** 2.4 What is the key difference between the macro average F1 score and the weighted average F1 score?\n",
        "\n",
        "**QUESTION:** 2.5 What is the macro average F1 score you get from the classification report?\n",
        "\n",
        "Now we'll generate another very valuable visualization of what's happening with our classifier -- a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udwdL6iKEh7b"
      },
      "outputs": [],
      "source": [
        "cm = tf.math.confusion_matrix(test_labels, predictions_model1)\n",
        "cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9YL2AlQOY4Y"
      },
      "source": [
        "And now we'll display it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh0Bxg-8EhzZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True,\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OKhCGVDOgdl"
      },
      "source": [
        "### 3. Classification using two stages\n",
        "\n",
        "Okay, not bad.  As you can see, some categories are easier to distinguish than others. Look for the class with the lowest F1 score (it should be the one at the bottom of the list). In the confusion matrix, which other class is that one being mistaken for most often?\n",
        "\n",
        "You might notice that the categories in this dataset are somewhat heirarchical. There are more obvious differences between groups of news categories (e.g. computers vs recreation) and then subtler differences between categories within those groups (e.g. PC vs Mac, within computers).\n",
        "\n",
        "When this happens, one idea is to train a series of models, to first separate out the more obvious groups of classes, and then use more specialized sub-models to classify only a subset of the classes. Let's try that here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qA4QBstsin"
      },
      "source": [
        "#### Step 1: New model with 19 classes\n",
        "\n",
        "For simplicity, we'll just combine two categories in our first step. We'll replace the label of the last class with the label of the class it's most often mistaken for. (That way, we'll have labels from 0 to 18 instead of 0 to 19, and don't have to renumber everything, though you would have to if you group them more.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0rAv6nLmrwN"
      },
      "outputs": [],
      "source": [
        "label_to_replace = 19\n",
        "\n",
        "# label_to_replace_with = ...\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "train_labels_19class = train_labels.copy()\n",
        "train_labels_19class[train_labels_19class == label_to_replace] = label_to_replace_with\n",
        "\n",
        "valid_labels_19class = npvalid_labels.copy()\n",
        "valid_labels_19class[valid_labels_19class == label_to_replace] = label_to_replace_with\n",
        "\n",
        "test_labels_19class = test_labels.copy()\n",
        "test_labels_19class[test_labels_19class == label_to_replace] = label_to_replace_with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSQd_UDQhmQd"
      },
      "source": [
        "Now let's create a new model with the same architecture, but to predict probabilities for 19 classes instead of 20. We're using all of the data in this first step, so we'll use the encodings we already preprocessed as inputs, but use the new labels that only have 19 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7hR7ahmmrYL"
      },
      "outputs": [],
      "source": [
        "bert_model_19class = create_bert_multiclass_model(checkpoint = model_checkpoint, num_classes=19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Qsai6UmrOj"
      },
      "outputs": [],
      "source": [
        "bert_model_19class_history = bert_model_19class.fit([train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
        "                                                  train_labels_19class,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, valid_encodings.token_type_ids, valid_encodings.attention_mask],\n",
        "                                                                   valid_labels_19class),    \n",
        "                                                  batch_size=8,\n",
        "                                                  epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkW9hMq9g9dV"
      },
      "outputs": [],
      "source": [
        "#Evaluate the fine tuned 19-class model against the test data with 19-class labels\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE\n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaUdul4hlp0"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.1 What is the test accuracy you get when you run the new first stage model with only 19 classes?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrWtaXSug9UR"
      },
      "outputs": [],
      "source": [
        "#run and capture all the predictions from the 19 class data\n",
        "### YOUR CODE HERE \n",
        "### END YOUR CODE\n",
        "\n",
        "predictions_19class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nAlDV7K8T4q"
      },
      "outputs": [],
      "source": [
        "target_names_19class = target_names[:label_to_replace_with] \\\n",
        "                     + ['** COMBINED CLASS **'] \\\n",
        "                     + target_names[label_to_replace_with+1:19]\n",
        "\n",
        "print(classification_report(test_labels_19class, predictions_19class.numpy(),\n",
        "                            target_names=target_names_19class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh6nKilF8V7b"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.2 What is the F1 score you get for the combined class when you run the new first stage model with only 19 classes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZcBibxR153"
      },
      "source": [
        "#### Step 2: New model with only the two classes combined in step one\n",
        "\n",
        "Now, our first stage model is able to determine which text is one of the two often confused classes, but we need to train a more specific model to distinguish between just these two classes. Ideally, this model will only focus on the more subtle differences between these two news categories, since it doesn't have to learn everything else about the other categories.\n",
        "\n",
        "For this model, we're only going to train using the text examples that are one of the two confused categories. We'll keep the encodings we already tokenized, so we need to separate out the input_ids, token_type_ids, and attention_mask for just the rows that have one of these two labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4TmPG6B1mz0"
      },
      "outputs": [],
      "source": [
        "train_mask_2class = (train_labels_19class == label_to_replace_with)\n",
        "train_encodings_2class = {'input_ids': train_encodings.input_ids[train_mask_2class],\n",
        "                          'token_type_ids': train_encodings.token_type_ids[train_mask_2class],\n",
        "                          'attention_mask': train_encodings.attention_mask[train_mask_2class]}\n",
        "train_labels_2class = train_labels.copy()[train_mask_2class]\n",
        "train_labels_2class = (train_labels_2class == label_to_replace_with).astype(int)\n",
        "\n",
        "valid_mask_2class = (valid_labels_19class == label_to_replace_with)\n",
        "valid_encodings_2class = {'input_ids': valid_encodings.input_ids[valid_mask_2class],\n",
        "                          'token_type_ids': valid_encodings.token_type_ids[valid_mask_2class],\n",
        "                          'attention_mask': valid_encodings.attention_mask[valid_mask_2class]}\n",
        "valid_labels_2class = npvalid_labels.copy()[valid_mask_2class]\n",
        "valid_labels_2class = (valid_labels_2class == label_to_replace_with).astype(int)\n",
        "\n",
        "test_mask_2class = (test_labels_19class == label_to_replace_with)\n",
        "test_encodings_2class = {'input_ids': test_encodings.input_ids[test_mask_2class],\n",
        "                          'token_type_ids': test_encodings.token_type_ids[test_mask_2class],\n",
        "                          'attention_mask': test_encodings.attention_mask[test_mask_2class]}\n",
        "test_labels_2class = test_labels.copy()[test_mask_2class]\n",
        "test_labels_2class = (test_labels_2class == label_to_replace_with).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwTzhDVp9YTT"
      },
      "outputs": [],
      "source": [
        "train_labels_2class.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O76HzIGh9dct"
      },
      "outputs": [],
      "source": [
        "train_labels_2class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c7M7sFCucU8"
      },
      "source": [
        "Create and train a new model with the same architecture as before, except that it only predicts two classes. (Note that we could change this to a binary prediction model, but we'll keep it multiclass for consistency here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNe5zVOflttk"
      },
      "outputs": [],
      "source": [
        "bert_model_2class = create_bert_multiclass_model(checkpoint=model_checkpoint, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F5DM3zGlvSd"
      },
      "outputs": [],
      "source": [
        "bert_model_2class_history = bert_model_2class.fit([train_encodings_2class['input_ids'],\n",
        "                                                   train_encodings_2class['token_type_ids'],\n",
        "                                                   train_encodings_2class['attention_mask']], \n",
        "                                                  train_labels_2class,   \n",
        "                                                  validation_data=([valid_encodings_2class['input_ids'],\n",
        "                                                                    valid_encodings_2class['token_type_ids'],\n",
        "                                                                    valid_encodings_2class['attention_mask']],\n",
        "                                                                   valid_labels_2class),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNEUeufxp1g5"
      },
      "outputs": [],
      "source": [
        "#Evaluate the two-class model against the two-class test set.\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE\n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPuN8_gaESJZ"
      },
      "outputs": [],
      "source": [
        "#run and capture all the predictions from the 2-class test data\n",
        "### YOUR CODE HERE \n",
        "### END YOUR CODE\n",
        "predictions_2class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEnHd3ijESPH"
      },
      "outputs": [],
      "source": [
        "# Run the sklearn classification_report again with the 2-class predictions\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTDW3JBut1P"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.3 What is the macro average F1 score you get when you run the new second stage model with only 2 classes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90DMNd_Nuzgz"
      },
      "source": [
        "#### Step 3: Combine the predicted labels from the two steps\n",
        "\n",
        "To combine our models into two steps, start with the predictions from the first step. Keep all predicted labels except the ones with a predicted value of label_to_replace_with (the label we gave to both of the confused classes in the first step).\n",
        "\n",
        "Wherever the first model predicted the combined category, we'll replace the predictions with the label from the second model. If we used these models in inference, we'd only send an example to the second model if the first model predicted that it was from the combined class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmSd5B8M-YPh"
      },
      "outputs": [],
      "source": [
        "predictions_2class = predictions_2class.numpy()\n",
        "predictions_2class[predictions_2class == 0] = label_to_replace\n",
        "predictions_2class[predictions_2class == 1] = label_to_replace_with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czf7y8Ha5hu-"
      },
      "outputs": [],
      "source": [
        "predictions_2steps = predictions_19class.numpy()\n",
        "predictions_2steps[test_mask_2class] = predictions_2class\n",
        "\n",
        "predictions_2steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxUBa20mv0Iq"
      },
      "source": [
        "Now let's look at the classification report and confusion matrix, using the combined predictions from our two step model (compared to the original labels). Did the overall results get better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbtOD45X6QgO"
      },
      "outputs": [],
      "source": [
        "# Run the sklearn classification_report with all 20 classes from the 2-step predictions\n",
        "### YOUR CODE HERE\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoxOfneGDG60"
      },
      "outputs": [],
      "source": [
        "cm = tf.math.confusion_matrix(test_labels, predictions_2steps)\n",
        "cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of7FDh3CDHGy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True,\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jXBmgS2iXDI"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "3.4 What is the macro average F1 score you get from the combined two-step model?\n",
        "\n",
        "3.5 What is the difference in points between the macro weighted F1 score for the original model and the combined two-step model?\n",
        "\n",
        "3.6 What is the new F1 score for the last category (i.e. label_to_replace, the one that had the lowest F1 score in the original model)?\n",
        "\n",
        "3.7 What is the new F1 score for the other category that you combined with the last category in the two-step model (i.e. label_to_replace_with)?\n",
        "\n",
        "3.8 Which metric (precision or recall) is now lower for the other category (i.e. label_to_replace_with)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyVPop8SuG9"
      },
      "source": [
        "### Look at examples of misclassifications\n",
        "\n",
        "What happened in the two-step model? Did everything improve, or did something get worse? We were concerned about the last news category, which had a very low F1 score in the original model. In the two-step model, the F1 score for that category should have gone up.\n",
        "\n",
        "But for the other category that the original model often confused with the last category, the F1 score might have gone down. In particular, one of the two component metrics, precision or recall, probably went down. (We ask you which one went down in question 3.7 above.)\n",
        "\n",
        "We might be able to tell what happened from the confusion matrix, but it's also always a good idea to look at actual examples that were misclassified, to see if we can spot any patterns. We can also isolate more specific examples, like test examples that the original model got right, but the two-step model got wrong. Let's do that below.\n",
        "\n",
        "**CRITICAL NOTE:**  If nothing prints out when you run the code below, there are two possibilities.  The first is that there is some error in the code or variable names you have created in earlier cells.  The second possibility is that given your current train, validation, and test split, the second model predicted the \"label_to_replace_with\" class and the first model did so too.  This is unlikely but it is possible. In either case, you must go back and re-run the *ENTIRE* notebook to make sure you get a new train, validation, and test split which will allow you to observe the first and second models disagreeing. Please make sure you enter the metric values from this new run into your answers file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj98aBdmNMrE"
      },
      "outputs": [],
      "source": [
        "# Make a vector the length of our test set, with 1 if the second model predicted the\n",
        "# \"label_to_replace_with\" class, and 0s otherwise\n",
        "select_predictions = (predictions_2steps == label_to_replace_with)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaBVzlNXdsWN"
      },
      "outputs": [],
      "source": [
        "# Now only keep a 1 if that was not the correct label, i.e. it was a false positive\n",
        "select_predictions = select_predictions * (test_labels != label_to_replace_with)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E05K9v-RdsLT"
      },
      "outputs": [],
      "source": [
        "# And now only keep a 1 if the original model predicted the correct label instead\n",
        "select_predictions = select_predictions * (test_labels == predictions_model1.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dEwbnc_6qr"
      },
      "outputs": [],
      "source": [
        "# Print out the original and clean text of the examples that met the above conditions\n",
        "for i in np.where(select_predictions)[0]:\n",
        "\n",
        "    print('Prediction: model1 = %s, model2 = %s):\\nText: %s\\n\\n' %\n",
        "          (target_names[predictions_model1[i]],\n",
        "           target_names[predictions_2steps[i]],\n",
        "           test_texts[i][:1000].replace('\\n', ' ')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppncPlCzzz0B"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "4.1 Why do you think the two-step model got these examples wrong, when the original model got them right?\n",
        "\n",
        "- A. The two-step model saw less examples of the \"label_to_replace\" class, because we replaced them with the \"label_to_replace_with\" examples. So it didn't learn the kind of text in that class as well as the original model.\n",
        "\n",
        "- B. In the two-step process, the step 1 model overpredicted the combined class, and the step 2 model overpredicted the \"label_to_replace_with\" class. A third class is now getting mistaken more often for the \"label_to_replace_with\" class, than in the original model.\n",
        "\n",
        "- C. It's probably just random that the original model got these specific examples right and the two-step model got them wrong.\n",
        "\n",
        "\n",
        "\n",
        "4.2 Is there anything you might try next, to try to make the two-step model better?\n",
        "\n",
        "- A. Try to balance the training data across classes at each step, or add class weights when calling model.fit.\n",
        "\n",
        "- B. Try to combine another similar category with the two easily confused ones, for a step 1 model with 18 classes and the step 2 model with 3 classes.\n",
        "\n",
        "- C. Try both A and B"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}